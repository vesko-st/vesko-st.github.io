<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Veselin Stoyanov</title>
    <link rel="icon" type="image/x-icon" href="favicon-32x32.png">
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.10/css/all.css" integrity="sha384-+d0P83n9kaQMCwj8F4RJB66tzIwOKmrdb46+porD/OvrJ+37WqIM7UoBtwHO6Nlg" crossorigin="anonymous">
  </head>
  <body>
    <div class="wrapper">
      <header>
	  <p style="text-align: center"> <small>
		<a href="mailto:vesko.st@gmail.com"><span class="fas fa-envelope-open fa-fw"></span> e-mail</a> 
		<a href="https://tiny.one/sch-ves"><span class="fas fa-graduation-cap fa-fw"></span> Google Scholar</a> 
		<a href="http://www.linkedin.com/in/ves-sto"><span class="fab fa-linkedin fa-fw"></span> LinkedIn</a> 
		<a href="https://twitter.com/vesko_st"><span class="fab fa-twitter fa-fw"></span> Twitter</a>
	  </small></p>
        <p style="text-align: center"><img src="ves_2022_circle_small.png" width="150px" /></p>
        <h1 style="text-align: center">Veselin Stoyanov</h1>
        <p>Applied Research Leader with a track record of innovating in AI and NLP to solve real-world problems. 
		Lead teams to pretraining, multilingual NLP and large LM methods such as RoBERTa, XLM-R and MultiRay,
		which are now industry standards. Successfully used these models to improve online experiences, e.g. 
		reduce prevalence of hate speech and bullying posts. Experienced in building and motivating 
		high-performing diverse teams and mentoring researchers and engineers.</p>
      </header>
      <section>
        <h1>Experience</h1>
        <h2>Facebook / Meta Inc, Menlo Park, CA </h2>
        <dl>
          <dt style="color: green">Applied Research Scientist Manager</dt>
          <dd>Jul 2018 - Nov 2022</dd>
          <dt style="color: green">Research Scientist</dt>
          <dd>Jan 2013 - Jul 2018</dd>
        </dl>
        <h3> Project Highlights </h3>
	  <ul>
	    <li><strong style="color: green"> MultiRay </strong><br> 
		Built a service to run multiple very large and accurate models on the same input, and share the 
		majority of the computational costs. MultiRay makes it possible for very accurate self-supervised 
		models to be run on every piece of content. (<a href="https://arxiv.org/abs/2004.14287">paper</a>, 
		<a href="https://ai.facebook.com/blog/multiray-large-scale-AI-models/">blog</a>) 
	    </li>
	    <li><strong style="color: green"> Cross-lingual NLP through XLM-R </strong><br>
		Trained XLM-R, a state-of-the-art large-scale multilingual language model 
		(<a href="https://arxiv.org/abs/1911.02116">paper</a>, <a href="https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/">blog</a>)
		and applied it to extend Integrity classifiers to many languages (<a href="https://ai.facebook.com/blog/how-ai-is-getting-better-at-detecting-hate-speech/">blog</a>). 
		Extended upon previous work on multilingual word embeddings (<a href="https://ai.facebook.com/blog/under-the-hood-multilingual-embeddings/">blog</a>).
	    </li>
          <li><strong style="color: green"> RoBERTa and applications to Integrity </strong><br> 
		Trained RoBERTa, a robustly optimized BERT pretraining approach, a state-of-the-art 
		self-supervised method (<a href="https://arxiv.org/abs/1907.11692">paper</a>, 
		<a href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/">blog</a>, 
		<a href="https://ai.facebook.com/blog/new-advances-in-natural-language-processing-to-better-connect-people/">blog</a>). Applied it to identifying violations such as 
		hate speech (<a href="https://ai.facebook.com/blog/ai-advances-to-better-detect-hate-speech/">blog</a>) and bullying. (<a href="https://arxiv.org/abs/2009.10311">paper</a>, <a href="https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people/">blog</a>) 
	    </li>
          <li><strong style="color: green"> Neural Machine Translation </strong><br>
		Shipped the first large-scale commercial Neural MT system with big improvements to translation 
		quality. (<a href="https://engineering.fb.com/2017/08/03/ml-applications/transitioning-entirely-to-neural-machine-translation/">blog</a>, <a href="https://techcrunch.com/2017/08/03/facebook-finishes-its-move-to-neural-machine-translation/">news</a>)
	    </li>
          <li><strong style="color: green"> NLP for Search </strong><br>
		Shipped several impactful NLP features to Facebook Search including phonetic name search, 
		intent classification and keyword typeahead.
	    </li>
	  </ul>
        <h2>Center for Language and Speech Processing (CLSP), Johns Hopkins University </h2>
        <dl>
          <dt style="color: green">Assistant Research Scientist</dt>
          <dd>Oct 2010 - Jan 2013</dd>
        </dl>
	<ul>
	    <li>Computing Innovation Fellowship (awarded by CRA). 
	    </li>
	    <li>Performed research on Machine Learning for Structured Prediction.
	    </li>
	</ul>
      <h1>Education</h1>
        <h2>Cornell University, Ithaca, NY </h2>
        <dl>
          <dt style="color: green">PhD in Computer Science</dt>
          <dd>Aug 2010</dd>
          <dt style="color: green">MSc in Computer Science</dt>
          <dd>Aug 2006</dd>
        </dl>
	  <p> Advisor: Prof. Claire Cardie. 
		Thesis title: Opinion Summarization: Automatically Creating Useful Representations of Opinions Expressed in Text.</p>
        <h2>University of Delaware, Newark, DE </h2>
        <dl>
          <dt style="color: green">Honors BSc, with Distinction in Computer Science</dt>
          <dd>May 2002</dd>
        </dl>
	  <p> Graduated Summa Cum Laude; GPA: 4.00/4.00. Minors in Mathematics and Cognitive Science. </p>
      <h1>Selected Publications</h1>
	<p> Full publication list available on <a href="https://tiny.one/sch-ves"><span class="fas fa-graduation-cap fa-fw"></span> Google Scholar</a> 
</p>
	<ul>
	    <li> <a href="https://arxiv.org/pdf/1907.11692.pdf%5C"> RoBERTa: A Robustly Optimized BERT Pretraining Approach </a> <br/>
	    Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov 
	    </li>
	    <li> <a href="https://arxiv.org/pdf/1910.13461">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension </a> <br/>
	    Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer
	    </li>
	    <li> <a href="https://arxiv.org/pdf/1911.02116">Unsupervised Cross-lingual Representation Learning at Scale </a> <br/>
	    Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
	    </li>
	    <li> <a href="https://arxiv.org/pdf/1809.05053"> XNLI: Evaluating Cross-lingual Sentence Representations </a> <br/>
	    Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, Veselin Stoyanov
	    </li>
	    <li> <a href="https://arxiv.org/pdf/2011.01403"> Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning</a> <br/>
	    Beliz Gunel, Jingfei Du, Alexis Conneau, Veselin Stoyanov
	    </li>
	    <li> <a href="https://arxiv.org/abs/1911.01464">Emerging Cross-lingual Structure in Pretrained Language Models </a> <br/>
	    Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov
	    </li>
	    <li> <a href="https://arxiv.org/pdf/1912.09637">Pretrained Encyclopedia: Weakly supervised knowledge-pretrained language model</a> <br/>
	    Wenhan Xiong, Jingfei Du, William Wang, Veselin Stoyanov
	    </li>
	    <li> <a href="https://dl.acm.org/doi/fullHtml/10.1145/3462671">Preserving integrity in online social networks </a> <br/>
	    Alon Halevy, Cristian Canton-Ferrer, Hao Ma, Umut Ozertem, Patrick Pantel, Marzieh Saeidi, Fabrizio Silvestri, Veselin Stoyanov
	    </li>
	    <li> <a href="http://proceedings.mlr.press/v15/stoyanov11a/stoyanov11a.pdf"> Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure</a> <br/>
	    Veselin Stoyanov, Alexander Ropson, Jason Eisner
	    </li>
	    <li> <a href="https://aclanthology.org/P09-1074.pdf"> Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art</a> <br/>
	    Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen Riloff
	    </li>
	</ul>
	<p> Full publication list available on <a href="https://tiny.one/sch-ves"><span class="fas fa-graduation-cap fa-fw"></span> Google Scholar</a> 
</p>
	
<!--
        <p><em>View the <a href="http://github.github.com/github-flavored-markdown/sample_content.html">source of this content</a>.</em></p>

        <p>Let's get the whole "linebreak" thing out of the way. The next paragraph contains two phrases separated by a single newline character:</p>

        <p>Roses are red<br>
        Violets are blue</p>

        <p>The next paragraph has the same phrases, but now they are separated by two spaces and a newline character:</p>

        <p>Roses are red<br><br>
        Violets are blue</p>

        <p>Oh, and one thing I cannot stand is the mangling of words with multiple underscores in them like perform_complicated_task or do_this_and_do_that_and_another_thing.</p>

        <h2>A bit of the GitHub spice</h2>

        <p>In addition to the changes in the previous section, certain references are auto-linked:</p>

        <ul>
          <li>SHA: be6a8cc1c1ecfe9489fb51e4869af15a13fc2cd2</li>
          <li>User@SHA ref: <a href="https://github.com/mojombo/product/commit/be6a8cc1c1ecfe9489fb51e4869af15a13fc2cd2" class="commit-link">mojombo@<tt>be6a8cc</tt></a></li>
          <li>User/Project@SHA: <a href="https://github.com/mojombo/god/commit/be6a8cc1c1ecfe9489fb51e4869af15a13fc2cd2" class="commit-link">mojombo/god@<tt>be6a8cc</tt></a></li>
          <li>#Num: <a href="https://github.com/github/product/issues/1" class="issue-link" title="Baseline: What is up in the air right now?">#1</a></li>
          <li>User/#Num: mojombo#1</li>
          <li>User/Project#Num: <a href="https://github.com/mojombo/god/issues/1" class="issue-link" title="The server is not available (or you do not have permissions to access it)">mojombo/god#1</a></li>
        </ul>

        <p>These are dangerous goodies though, and we need to make sure email addresses don't get mangled:</p>

        <p>My email addy is <a href="mailto:tom@github.com">tom@github.com</a>.</p>

        <h2>Math is hard, let's go shopping</h2>

        <p>In first grade I learned that 5 &gt; 3 and 2 &lt; 7. Maybe some arrows. 1 -&gt; 2 -&gt; 3. 9 &lt;- 8 &lt;- 7.</p>

        <p>Triangles man! a^2 + b^2 = c^2</p>

        <h2>We all like making lists</h2>

        <p>The above header should be an H2 tag. Now, for a list of fruits:</p>

        <ul>
          <li>Red Apples</li>
          <li>Purple Grapes</li>
          <li>Green Kiwifruits</li>
        </ul>

        <p>Let's get crazy:</p>

        <ol>
          <li>
            <p>This is a list item with two paragraphs. Lorem ipsum dolor<br>
            sit amet, consectetuer adipiscing elit. Aliquam hendrerit<br>
            mi posuere lectus.</p>

            <p>Vestibulum enim wisi, viverra nec, fringilla in, laoreet<br>
            vitae, risus. Donec sit amet nisl. Aliquam semper ipsum<br>
            sit amet velit.</p>
          </li>
          <li><p>Suspendisse id sem consectetuer libero luctus adipiscing.</p></li>
        </ol>

        <p>What about some code <strong>in</strong> a list? That's insane, right?</p>

        <ol>
          <li>
            <p>In Ruby you can map like this:</p>

            <pre><code>['a', 'b'].map { |x| x.uppercase }</code></pre>
          </li>
          <li>
            <p>In Rails, you can do a shortcut:</p>

            <pre><code>['a', 'b'].map(&amp;:uppercase)</code></pre>
          </li>
        </ol>

        <p>Some people seem to like definition lists</p>

        <h2>I am a robot</h2>

        <p>Maybe you want to print <code>robot</code> to the console 1000 times. Why not?</p>

        <pre><code>def robot_invasion
  puts("robot " * 1000)
end
</code></pre>

        <p>You see, that was formatted as code because it's been indented by four spaces.</p>

        <p>How about we throw some angle braces and ampersands in there?</p>

        <pre><code>&lt;div class="footer"&gt;
    &amp;copy; 2004 Foo Corporation
&lt;/div&gt;
</code></pre>

        <h2>Set in stone</h2>

        <p>Preformatted blocks are useful for ASCII art:</p>

        <pre>             ,-.
    ,     ,-.   ,-.
   / \   (   )-(   )
   \ |  ,.&gt;-(   )-&lt;
    \|,' (   )-(   )
     Y ___`-'   `-'
     |/__/   `-'
     |
     |
     |    -hrr-
  ___|_____________
</pre>

        <h2>Playing the blame game</h2>

        <p>If you need to blame someone, the best way to do so is by quoting them:</p>

        <blockquote>
        <p>I, at any rate, am convinced that He does not throw dice.</p>
        </blockquote>

        <p>Or perhaps someone a little less eloquent:</p>

        <blockquote>
        <p>I wish you'd have given me this written question ahead of time so I<br>
        could plan for it... I'm sure something will pop into my head here in<br>
        the midst of this press conference, with all the pressure of trying to<br>
        come up with answer, but it hadn't yet...</p>

        <p>I don't want to sound like<br>
        I have made no mistakes. I'm confident I have. I just haven't - you<br>
        just put me under the spot here, and maybe I'm not as quick on my feet<br>
        as I should be in coming up with one.</p>
        </blockquote>

        <h2>Table for two</h2>

        <table>
          <tbody>
        <tr>
            <th>ID</th>
        <th>Name</th>
        <th>Rank</th>
          </tr>
          <tr>
            <td>1</td>
        <td>Tom Preston-Werner</td>
        <td>Awesome</td>
          </tr>
          <tr>
            <td>2</td>
        <td>Albert Einstein</td>
        <td>Nearly as awesome</td>
          </tr>
        </tbody>
        </table>

        <h2>Crazy linking action</h2>

        <p>I get 10 times more traffic from <a href="http://google.com/" title="Google">Google</a> than from<br>
        <a href="http://search.yahoo.com/" title="Yahoo Search">Yahoo</a> or <a href="http://search.msn.com/" title="MSN Search">MSN</a>.</p>
-->
	  <br><br><br>
	  <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
